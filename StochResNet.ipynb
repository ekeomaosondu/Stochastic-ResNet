{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d5f487",
   "metadata": {},
   "source": [
    "In this notebook, we will implement a Stochastic ResNet architecture for image classification. Stochastic ResNets introduce randomness in the forward pass by propagating uncertainty alongside features, enabling better regularization and improved generalization through stochastic depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a113e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATASET = \"CIFAR100\"  # or \"CIFAR100\"\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "LR = 1e-3\n",
    "\n",
    "# Beta (KL weight) schedule\n",
    "# Overall loss: loss = CE + beta * KL\n",
    "BETA_MAX = 1e-4              # max beta\n",
    "BETA_SCHEDULE = \"linear_warmup\"  # options: \"constant\", \"linear_warmup\"\n",
    "BETA_WARMUP_FRAC = 0.3       # fraction of epochs used to warm up (for linear_warmup)\n",
    "\n",
    "# MC sampling for evaluation & uncertainty\n",
    "T_MC_EVAL = 20          # MC passes for test accuracy\n",
    "T_MC_VIZ = 20           # MC passes for uncertainty viz\n",
    "\n",
    "# Checkpointing (save to Google Drive)\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive\")        # your Google Drive root\n",
    "CHECKPOINT_DIR = DRIVE_ROOT / \"stoch_resnet_ckpts\" # folder inside Drive\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAVE_EVERY_EPOCHS = 25  # save every N epochs\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "\n",
    "\n",
    "# Beta schedule helper\n",
    "def beta_schedule(epoch, num_epochs, schedule_type=\"linear_warmup\",\n",
    "                  beta_max=1e-4, warmup_frac=0.3):\n",
    "    \"\"\"\n",
    "    Returns beta for a given epoch index (0-based).\n",
    "    schedule_type:\n",
    "        - \"constant\"      : always beta_max\n",
    "        - \"linear_warmup\" : linearly increase from 0 -> beta_max over warmup_frac * num_epochs\n",
    "        - \"cosine\"        : cosine schedule between 0 and beta_max\n",
    "        - \"exp_decay\"     : start at beta_max, decay exponentially\n",
    "    \"\"\"\n",
    "    e = epoch\n",
    "    T = num_epochs\n",
    "    if schedule_type == \"constant\":\n",
    "        return beta_max\n",
    "\n",
    "    elif schedule_type == \"linear_warmup\":\n",
    "        warmup_epochs = max(1, int(T * warmup_frac))\n",
    "        if e < warmup_epochs:\n",
    "            return beta_max * ( (e + 1) / warmup_epochs )\n",
    "        else:\n",
    "            return beta_max\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77286ce",
   "metadata": {},
   "source": [
    "We now define a VIB (Variational Information Bottleneck) layer that will be used in our Stochastic ResNet. This layer will help us introduce uncertainty into the network while maintaining good feature representation. The VIB layer will output both deterministic features and stochastic uncertainty, enabling the network to learn robust representations through variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a289b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIBBlock(nn.Module):\n",
    "    def __init__(self, channels, prior=\"unit\", reduce=\"none\", sample_at_eval=False):\n",
    "        \"\"\"\n",
    "        prior:\n",
    "            \"unit\"  -> KL(N(x, σ^2) || N(0, I))  (penalizes mean and variance)\n",
    "            \"match\" -> KL(N(x, σ^2) || N(x, I))  (penalizes variance only)\n",
    "        reduce:\n",
    "            \"batch_mean\" -> average over batch (scalar)\n",
    "            \"none\"       -> per-sample KL [B]\n",
    "        sample_at_eval:\n",
    "            if True, still sample noise at eval (for MC uncertainty estimation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enc_logvar = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        nn.init.constant_(self.enc_logvar.weight, 0.0)\n",
    "        nn.init.constant_(self.enc_logvar.bias, -5.0)  # very low initial variance\n",
    "\n",
    "        self.prior = prior\n",
    "        self.reduce = reduce\n",
    "        self.sample_at_eval = sample_at_eval\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Predict log-variance\n",
    "        logvar = self.enc_logvar(x)\n",
    "        logvar = torch.clamp(logvar, min=-10.0, max=10.0)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "\n",
    "        # Sample or not\n",
    "        if self.training or self.sample_at_eval:\n",
    "            eps = torch.randn_like(std)\n",
    "            z = x + eps * std\n",
    "        else:\n",
    "            z = x\n",
    "\n",
    "        # KL computation\n",
    "        if self.prior == \"unit\":\n",
    "            # KL(N(μ=x, σ^2) || N(0, I))\n",
    "            kl_element = -0.5 * (1 + logvar - x.pow(2) - logvar.exp())\n",
    "        elif self.prior == \"match\":\n",
    "            # KL(N(μ=x, σ^2) || N(μ=x, I)) = 0.5 * (σ^2 - 1 - log σ^2)\n",
    "            kl_element = 0.5 * (logvar.exp() - 1.0 - logvar)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prior type: {self.prior}\")\n",
    "\n",
    "        # Sum over C, H, W → per-sample KL [B]\n",
    "        kl = kl_element.sum(dim=[1, 2, 3])  # [batch]\n",
    "\n",
    "        if self.reduce == \"batch_mean\":\n",
    "            kl = kl.mean()  # scalar\n",
    "\n",
    "        return z, kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353e166",
   "metadata": {},
   "source": [
    "Now, we define the Stochastic ResNet architecture that will use the VIB blocks. This architecture will consist of a series of residual blocks where some blocks incorporate the VIB layers to introduce stochasticity and uncertainty into the feature representations. The network will use stochastic depth to further enhance regularization and generalization. We will implement a modified ResNet backbone with VIB layers inserted at strategic points to enable uncertainty quantification while maintaining strong feature learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, sample_at_eval=False, prior=\"match\"):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "        # Adapt stem for CIFAR images (32x32)\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.backbone.maxpool = nn.Identity()\n",
    "\n",
    "        # inside StochasticResNet.__init__\n",
    "        self.vib1 = VIBBlock(64,  prior=prior, reduce=\"none\", sample_at_eval=sample_at_eval)\n",
    "        self.vib2 = VIBBlock(128, prior=prior, reduce=\"none\", sample_at_eval=sample_at_eval)\n",
    "        self.vib3 = VIBBlock(256, prior=prior, reduce=\"none\", sample_at_eval=sample_at_eval)\n",
    "        self.vib4 = VIBBlock(512, prior=prior, reduce=\"none\", sample_at_eval=sample_at_eval)\n",
    "\n",
    "\n",
    "        self.backbone.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x, kl1 = self.vib1(x)\n",
    "\n",
    "        x = self.backbone.layer2(x)\n",
    "        x, kl2 = self.vib2(x)\n",
    "\n",
    "        x = self.backbone.layer3(x)\n",
    "        x, kl3 = self.vib3(x)\n",
    "\n",
    "        x = self.backbone.layer4(x)\n",
    "        x, kl4 = self.vib4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.backbone.fc(x)\n",
    "\n",
    "        return logits, [kl1, kl2, kl3, kl4]"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
