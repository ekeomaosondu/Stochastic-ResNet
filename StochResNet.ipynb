{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3d5f487",
   "metadata": {},
   "source": [
    "In this notebook, we will implement a Stochastic ResNet architecture for image classification. Stochastic ResNets introduce randomness in the forward pass by propagating uncertainty alongside features, enabling better regularization and improved generalization through stochastic depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a113e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "DATASET = \"CIFAR100\"  # or \"CIFAR100\"\n",
    "\n",
    "# Hyperparameters\n",
    "BATCH_SIZE = 128\n",
    "TEST_BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 300\n",
    "\n",
    "LR = 1e-3\n",
    "\n",
    "# Beta (KL weight) schedule\n",
    "# Overall loss: loss = CE + beta * KL\n",
    "BETA_MAX = 1e-4              # max beta\n",
    "BETA_SCHEDULE = \"linear_warmup\"  # options: \"constant\", \"linear_warmup\"\n",
    "BETA_WARMUP_FRAC = 0.3       # fraction of epochs used to warm up (for linear_warmup)\n",
    "\n",
    "# MC sampling for evaluation & uncertainty\n",
    "T_MC_EVAL = 20          # MC passes for test accuracy\n",
    "T_MC_VIZ = 20           # MC passes for uncertainty viz\n",
    "\n",
    "# Checkpointing (save to Google Drive)\n",
    "DRIVE_ROOT = Path(\"/content/drive/MyDrive\")        # your Google Drive root\n",
    "CHECKPOINT_DIR = DRIVE_ROOT / \"stoch_resnet_ckpts\" # folder inside Drive\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "SAVE_EVERY_EPOCHS = 25  # save every N epochs\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Checkpoints will be saved to: {CHECKPOINT_DIR}\")\n",
    "\n",
    "\n",
    "# Beta schedule helper\n",
    "def beta_schedule(epoch, num_epochs, schedule_type=\"linear_warmup\",\n",
    "                  beta_max=1e-4, warmup_frac=0.3):\n",
    "    \"\"\"\n",
    "    Returns beta for a given epoch index (0-based).\n",
    "    schedule_type:\n",
    "        - \"constant\"      : always beta_max\n",
    "        - \"linear_warmup\" : linearly increase from 0 -> beta_max over warmup_frac * num_epochs\n",
    "        - \"cosine\"        : cosine schedule between 0 and beta_max\n",
    "        - \"exp_decay\"     : start at beta_max, decay exponentially\n",
    "    \"\"\"\n",
    "    e = epoch\n",
    "    T = num_epochs\n",
    "    if schedule_type == \"constant\":\n",
    "        return beta_max\n",
    "\n",
    "    elif schedule_type == \"linear_warmup\":\n",
    "        warmup_epochs = max(1, int(T * warmup_frac))\n",
    "        if e < warmup_epochs:\n",
    "            return beta_max * ( (e + 1) / warmup_epochs )\n",
    "        else:\n",
    "            return beta_max\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown schedule_type: {schedule_type}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77286ce",
   "metadata": {},
   "source": [
    "We now define a VIB (Variational Information Bottleneck) layer that will be used in our Stochastic ResNet. This layer will help us introduce uncertainty into the network while maintaining good feature representation. The VIB layer will output both deterministic features and stochastic uncertainty, enabling the network to learn robust representations through variational inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a289b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VIBBlock(nn.Module):\n",
    "    def __init__(self, channels, prior=\"unit\", reduce=\"none\", sample_at_eval=False):\n",
    "        \"\"\"\n",
    "        prior:\n",
    "            \"unit\"  -> KL(N(x, σ^2) || N(0, I))  (penalizes mean and variance)\n",
    "            \"match\" -> KL(N(x, σ^2) || N(x, I))  (penalizes variance only)\n",
    "        reduce:\n",
    "            \"batch_mean\" -> average over batch (scalar)\n",
    "            \"none\"       -> per-sample KL [B]\n",
    "        sample_at_eval:\n",
    "            if True, still sample noise at eval (for MC uncertainty estimation)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.enc_logvar = nn.Conv2d(channels, channels, kernel_size=1)\n",
    "        nn.init.constant_(self.enc_logvar.weight, 0.0)\n",
    "        nn.init.constant_(self.enc_logvar.bias, -5.0)  # very low initial variance\n",
    "\n",
    "        self.prior = prior\n",
    "        self.reduce = reduce\n",
    "        self.sample_at_eval = sample_at_eval\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Predict log-variance\n",
    "        logvar = self.enc_logvar(x)\n",
    "        logvar = torch.clamp(logvar, min=-10.0, max=10.0)\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "\n",
    "        # Sample or not\n",
    "        if self.training or self.sample_at_eval:\n",
    "            eps = torch.randn_like(std)\n",
    "            z = x + eps * std\n",
    "        else:\n",
    "            z = x\n",
    "\n",
    "        # KL computation\n",
    "        if self.prior == \"unit\":\n",
    "            # KL(N(μ=x, σ^2) || N(0, I))\n",
    "            kl_element = -0.5 * (1 + logvar - x.pow(2) - logvar.exp())\n",
    "        elif self.prior == \"match\":\n",
    "            # KL(N(μ=x, σ^2) || N(μ=x, I)) = 0.5 * (σ^2 - 1 - log σ^2)\n",
    "            kl_element = 0.5 * (logvar.exp() - 1.0 - logvar)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown prior type: {self.prior}\")\n",
    "\n",
    "        # Sum over C, H, W → per-sample KL [B]\n",
    "        kl = kl_element.sum(dim=[1, 2, 3])  # [batch]\n",
    "\n",
    "        if self.reduce == \"batch_mean\":\n",
    "            kl = kl.mean()  # scalar\n",
    "\n",
    "        return z, kl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5353e166",
   "metadata": {},
   "source": [
    "Now, we define the Stochastic ResNet architecture that will use the VIB blocks. This architecture will consist of a series of residual blocks where some blocks incorporate the VIB layers to introduce stochasticity and uncertainty into the feature representations. The network will use stochastic depth to further enhance regularization and generalization. We will implement a modified ResNet backbone with VIB layers inserted at strategic points to enable uncertainty quantification while maintaining strong feature learning capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5a2bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticResNet(nn.Module):\n",
    "    def __init__(self, num_classes=10, sample_at_eval=False, prior=\"match\"):\n",
    "        super().__init__()\n",
    "        self.backbone = torchvision.models.resnet18(pretrained=False)\n",
    "\n",
    "        # Adapt stem for CIFAR images (32x32)\n",
    "        self.backbone.conv1 = nn.Conv2d(\n",
    "            3, 64, kernel_size=3, stride=1, padding=1, bias=False\n",
    "        )\n",
    "        self.backbone.maxpool = nn.Identity()\n",
    "\n",
    "        # inside StochasticResNet.__init__\n",
    "        self.vib1 = VIBBlock(64,  prior=prior, reduce=\"none\", sample_at_eval=sample_at_eval)\n",
    "        self.vib2 = VIBBlock(128, prior=prior, reduce=\"none\", sample_at_eval=sample_at_eval)\n",
    "        self.vib3 = VIBBlock(256, prior=prior, reduce=\"none\", sample_at_eval=sample_at_eval)\n",
    "        self.vib4 = VIBBlock(512, prior=prior, reduce=\"none\", sample_at_eval=sample_at_eval)\n",
    "\n",
    "\n",
    "        self.backbone.fc = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.conv1(x)\n",
    "        x = self.backbone.bn1(x)\n",
    "        x = self.backbone.relu(x)\n",
    "        x = self.backbone.maxpool(x)\n",
    "\n",
    "        x = self.backbone.layer1(x)\n",
    "        x, kl1 = self.vib1(x)\n",
    "\n",
    "        x = self.backbone.layer2(x)\n",
    "        x, kl2 = self.vib2(x)\n",
    "\n",
    "        x = self.backbone.layer3(x)\n",
    "        x, kl3 = self.vib3(x)\n",
    "\n",
    "        x = self.backbone.layer4(x)\n",
    "        x, kl4 = self.vib4(x)\n",
    "\n",
    "        x = self.backbone.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        logits = self.backbone.fc(x)\n",
    "\n",
    "        return logits, [kl1, kl2, kl3, kl4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc51845",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, loader, optimizer, beta, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    total_kl = 0.0\n",
    "\n",
    "    for x, y in loader:\n",
    "        x = x.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        logits, kl_list = model(x)  # each kl in kl_list is [B]\n",
    "\n",
    "        ce_loss = F.cross_entropy(logits, y)\n",
    "        # mean over batch for each layer, then sum over layers\n",
    "        kl_loss = sum(kl.mean() for kl in kl_list)\n",
    "\n",
    "        loss = ce_loss + (beta * kl_loss)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        total_kl += kl_loss.item()\n",
    "\n",
    "    return total_loss / len(loader), total_kl / len(loader)\n",
    "\n",
    "def mc_predict(model, x, T=20):\n",
    "    \"\"\"\n",
    "    x: [B, C, H, W] tensor on the correct device.\n",
    "    Returns:\n",
    "      mean_probs: [B, num_classes]\n",
    "      probs_stacked: [T, B, num_classes]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    probs_list = []\n",
    "    with torch.no_grad():\n",
    "        for _ in range(T):\n",
    "            logits, _ = model(x)\n",
    "            probs = logits.softmax(dim=-1)\n",
    "            probs_list.append(probs)\n",
    "\n",
    "    probs_stacked = torch.stack(probs_list, dim=0)\n",
    "    mean_probs = probs_stacked.mean(dim=0)\n",
    "    return mean_probs, probs_stacked\n",
    "\n",
    "\n",
    "def evaluate_mc(model, loader, device, T=20):\n",
    "    \"\"\"MC test accuracy using T stochastic forward passes.\"\"\"\n",
    "    model.eval()\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            mean_probs, _ = mc_predict(model, x, T=T)\n",
    "            preds = mean_probs.argmax(dim=-1)\n",
    "            total += y.size(0)\n",
    "            correct += (preds == y).sum().item()\n",
    "\n",
    "    return correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa99cb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_uncertainty(\n",
    "    model, loader, device, T=20, max_batches=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Collect per-image:\n",
    "      - MC predictive entropy (output uncertainty)\n",
    "      - per-layer KL vector [L]\n",
    "      - total KL (sum over layers)\n",
    "    Returns:\n",
    "      images:     [N, 3, 32, 32]\n",
    "      labels:     [N]\n",
    "      entropies:  [N]\n",
    "      preds:      [N]\n",
    "      layer_kls:  [N, L]  (L = # of VIBBlocks)\n",
    "      total_kls:  [N]\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    all_images = []\n",
    "    all_labels = []\n",
    "    all_entropy = []\n",
    "    all_preds = []\n",
    "    all_layer_kls = []\n",
    "    all_total_kls = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for b, (x, y) in enumerate(loader):\n",
    "            if (max_batches is not None) and (b >= max_batches):\n",
    "                break\n",
    "\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            # MC predictive entropy\n",
    "            mean_probs, _ = mc_predict(model, x, T=T)\n",
    "            probs = mean_probs.clamp(min=1e-8)\n",
    "            entropy = -(probs * probs.log()).sum(dim=1)\n",
    "            preds = probs.argmax(dim=1)\n",
    "\n",
    "            # Single forward to get per-layer KLs [B] each\n",
    "            logits_det, kl_list = model(x)\n",
    "            layer_kls_batch = torch.stack(kl_list, dim=1)\n",
    "            total_kls_batch = layer_kls_batch.sum(dim=1)\n",
    "\n",
    "            # Collect\n",
    "            all_images.append(x.cpu())\n",
    "            all_labels.append(y.cpu())\n",
    "            all_entropy.append(entropy.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_layer_kls.append(layer_kls_batch.cpu())\n",
    "            all_total_kls.append(total_kls_batch.cpu())\n",
    "\n",
    "    images = torch.cat(all_images, dim=0)\n",
    "    labels = torch.cat(all_labels, dim=0)\n",
    "    entropies = torch.cat(all_entropy, dim=0)\n",
    "    preds = torch.cat(all_preds, dim=0)\n",
    "    layer_kls = torch.cat(all_layer_kls, dim=0)\n",
    "    total_kls = torch.cat(all_total_kls, dim=0)\n",
    "\n",
    "    return images, labels, entropies, preds, layer_kls, total_kls\n",
    "\n",
    "\n",
    "def compute_combined_uncertainty(entropies, total_kls, lambda_kl=0.5):\n",
    "    \"\"\"\n",
    "    entropies: [N]\n",
    "    total_kls: [N]\n",
    "    lambda_kl: weight on KL relative to entropy\n",
    "    Returns:\n",
    "      combined_scores: [N]\n",
    "    \"\"\"\n",
    "    # Normalize KL roughly to [0, 1] using min-max (robust-ish)\n",
    "    kl_min = total_kls.min()\n",
    "    kl_max = total_kls.max()\n",
    "    kl_range = (kl_max - kl_min).clamp(min=1e-8)\n",
    "    kl_norm = (total_kls - kl_min) / kl_range  # [0, 1]\n",
    "\n",
    "    combined = entropies + lambda_kl * kl_norm\n",
    "    return combined\n",
    "\n",
    "\n",
    "def visualize_uncertainty_groups(\n",
    "    images,\n",
    "    labels,\n",
    "    entropies,\n",
    "    preds,\n",
    "    layer_kls,\n",
    "    total_kls,\n",
    "    num_per_group=16,\n",
    "    dataset_name=\"CIFAR\",\n",
    "    lambda_kl=0.5,\n",
    "):\n",
    "    \"\"\"\n",
    "    Make 3 grids:\n",
    "      - lowest combined (most certain)\n",
    "      - medium\n",
    "      - highest combined (most uncertain)\n",
    "    Each image title shows:\n",
    "      pred, entropy, total KL, and layer-wise KL summary.\n",
    "    \"\"\"\n",
    "    N, L = layer_kls.shape  # L = num layers (e.g., 4)\n",
    "    combined = compute_combined_uncertainty(entropies, total_kls, lambda_kl=lambda_kl)\n",
    "\n",
    "    # sort ascending (low to high uncertainty)\n",
    "    sorted_indices = torch.argsort(combined)\n",
    "\n",
    "    k = min(num_per_group, N // 3)\n",
    "    idx_certain = sorted_indices[:k]\n",
    "    idx_uncertain = sorted_indices[-k:]\n",
    "    mid_start = (N - k) // 2\n",
    "    idx_middle = sorted_indices[mid_start:mid_start + k]\n",
    "\n",
    "    def make_titles(idxs):\n",
    "        titles = []\n",
    "        for i in idxs:\n",
    "            ent = float(entropies[i].item())\n",
    "            tot_kl = float(total_kls[i].item())\n",
    "            pred = int(preds[i].item())\n",
    "            kl_vec = layer_kls[i].tolist()\n",
    "            # Option: show only coarse summary to keep titles short\n",
    "            titles.append(\n",
    "                f\"pred={pred}, H={ent:.2f}, KLtot={tot_kl:.2f}, KL={['%.2f'%v for v in kl_vec]}\"\n",
    "            )\n",
    "        return titles\n",
    "\n",
    "    def show_image_grid(images, titles, n_rows=4, n_cols=4, fig_title=None):\n",
    "        Ngrid = n_rows * n_cols\n",
    "        images = images[:Ngrid]\n",
    "        titles = titles[:Ngrid]\n",
    "\n",
    "        fig, axes = plt.subplots(n_rows, n_cols, figsize=(2.5*n_cols, 2.5*n_rows))\n",
    "        axes = axes.flatten()\n",
    "\n",
    "        for i, ax in enumerate(axes):\n",
    "            if i >= len(images):\n",
    "                ax.axis(\"off\")\n",
    "                continue\n",
    "            img = images[i].permute(1, 2, 0).numpy()\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(titles[i], fontsize=7)\n",
    "            ax.axis(\"off\")\n",
    "\n",
    "        if fig_title is not None:\n",
    "            fig.suptitle(fig_title, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    # Most certain\n",
    "    show_image_grid(\n",
    "        images[idx_certain],\n",
    "        make_titles(idx_certain),\n",
    "        n_rows=int(np.sqrt(k)),\n",
    "        n_cols=int(np.ceil(k / np.sqrt(k))),\n",
    "        fig_title=f\"{dataset_name}: Most certain (low entropy & KL)\"\n",
    "    )\n",
    "\n",
    "    # Medium\n",
    "    show_image_grid(\n",
    "        images[idx_middle],\n",
    "        make_titles(idx_middle),\n",
    "        n_rows=int(np.sqrt(k)),\n",
    "        n_cols=int(np.ceil(k / np.sqrt(k))),\n",
    "        fig_title=f\"{dataset_name}: Medium combined uncertainty\"\n",
    "    )\n",
    "\n",
    "    # Most uncertain\n",
    "    show_image_grid(\n",
    "        images[idx_uncertain],\n",
    "        make_titles(idx_uncertain),\n",
    "        n_rows=int(np.sqrt(k)),\n",
    "        n_cols=int(np.ceil(k / np.sqrt(k))),\n",
    "        fig_title=f\"{dataset_name}: Most uncertain (high entropy & KL)\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8012805a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms (same for CIFAR-10 and CIFAR-100)\n",
    "transform_train = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomCrop(32, padding=4),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "transform_test = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "if DATASET.upper() == \"CIFAR10\":\n",
    "    train_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR10(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    num_classes = 10\n",
    "elif DATASET.upper() == \"CIFAR100\":\n",
    "    train_dataset = torchvision.datasets.CIFAR100(\n",
    "        root=\"./data\", train=True, download=True, transform=transform_train\n",
    "    )\n",
    "    test_dataset = torchvision.datasets.CIFAR100(\n",
    "        root=\"./data\", train=False, download=True, transform=transform_test\n",
    "    )\n",
    "    num_classes = 100\n",
    "else:\n",
    "    raise ValueError(\"DATASET must be 'CIFAR10' or 'CIFAR100'\")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True\n",
    ")\n",
    "\n",
    "model = StochasticResNet(\n",
    "    num_classes=num_classes,\n",
    "    sample_at_eval=True,   # important for MC prediction\n",
    "    prior=\"match\",\n",
    ").to(DEVICE)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "\n",
    "print(f\"Using dataset: {DATASET} with {num_classes} classes.\")\n",
    "print(\"Model and optimizer initialized.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
